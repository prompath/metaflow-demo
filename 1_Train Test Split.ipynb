{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92b4502-3948-41ea-ba24-d20670814f6f",
   "metadata": {},
   "source": [
    "# Load and Split Data Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c87bf1ad-1e1d-44fa-8e42-ad205d3df0c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-22T18:48:07.849348Z",
     "iopub.status.busy": "2025-02-22T18:48:07.848509Z",
     "iopub.status.idle": "2025-02-22T18:48:15.701252Z",
     "shell.execute_reply": "2025-02-22T18:48:15.700875Z",
     "shell.execute_reply.started": "2025-02-22T18:48:07.849289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metaflow 2.13.9 executing DataLoaderFlow for user:jeera\n",
      "Validating your flow...\n",
      "    The graph looks good!\n",
      "Running pylint...\n",
      "    Pylint is happy!\n",
      "2025-02-23 01:48:11.249 Workflow starting (run-id 1740250091248933):\n",
      "2025-02-23 01:48:11.261 [1740250091248933/start/1 (pid 87457)] Task is starting.\n",
      "2025-02-23 01:48:11.894 [1740250091248933/start/1 (pid 87457)] Running data loading and splitting flow...\n",
      "2025-02-23 01:48:11.977 [1740250091248933/start/1 (pid 87457)] Task finished successfully.\n",
      "2025-02-23 01:48:11.983 [1740250091248933/load/2 (pid 87460)] Task is starting.\n",
      "2025-02-23 01:48:12.675 [1740250091248933/load/2 (pid 87460)] Task finished successfully.\n",
      "2025-02-23 01:48:12.681 [1740250091248933/split/3 (pid 87463)] Task is starting.\n",
      "2025-02-23 01:48:13.388 [1740250091248933/split/3 (pid 87463)] Task finished successfully.\n",
      "2025-02-23 01:48:13.394 [1740250091248933/persist/4 (pid 87466)] Task is starting.\n",
      "2025-02-23 01:48:14.071 [1740250091248933/persist/4 (pid 87466)] Task finished successfully.\n",
      "2025-02-23 01:48:14.077 [1740250091248933/end/5 (pid 87469)] Task is starting.\n",
      "2025-02-23 01:48:14.701 [1740250091248933/end/5 (pid 87469)] Split dataset saved to ./data\n",
      "2025-02-23 01:48:14.703 [1740250091248933/end/5 (pid 87469)] Shape of saved train set (120, 4)\n",
      "2025-02-23 01:48:14.703 [1740250091248933/end/5 (pid 87469)] Shape of saved test set (30, 4)\n",
      "2025-02-23 01:48:14.780 [1740250091248933/end/5 (pid 87469)] Task finished successfully.\n",
      "2025-02-23 01:48:14.780 Done!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from metaflow import FlowSpec, Parameter, step, NBRunner\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class DataLoaderFlow(FlowSpec):\n",
    "    test_size = Parameter(\n",
    "        name=\"test_size\",\n",
    "        help=\"Ratio of test set\",\n",
    "        default=0.2\n",
    "    )\n",
    "    random_state = Parameter(\n",
    "        name=\"random_state\",\n",
    "        help=\"Random seed\",\n",
    "        default=42\n",
    "    )\n",
    "    dataset_dir = Parameter(\n",
    "        name=\"dataset_dir\",\n",
    "        help=\"Path to save the splits\",\n",
    "        default=\"./data\"\n",
    "    )\n",
    "\n",
    "    @step\n",
    "    def start(self):\n",
    "        self.dataset_path = Path(self.dataset_dir).resolve()\n",
    "        print(\"Running data loading and splitting flow...\")\n",
    "        self.next(self.load)\n",
    "\n",
    "    @step\n",
    "    def load(self):\n",
    "        self.dataset = load_iris()\n",
    "        self.next(self.split)\n",
    "\n",
    "    @step\n",
    "    def split(self):\n",
    "        data = self.dataset[\"data\"]\n",
    "        target = self.dataset[\"target\"]\n",
    "        train_data, test_data, train_target, test_target = train_test_split(\n",
    "            data, \n",
    "            target, \n",
    "            test_size=self.test_size, \n",
    "            random_state=self.random_state, \n",
    "            stratify=target\n",
    "        )\n",
    "        self.train = pd.DataFrame(train_data, columns=self.dataset[\"feature_names\"])\n",
    "        self.train[\"target\"] = train_target\n",
    "        self.test = pd.DataFrame(test_data, columns=self.dataset[\"feature_names\"])\n",
    "        self.test[\"target\"] = test_target\n",
    "        self.next(self.persist)\n",
    "\n",
    "    @step\n",
    "    def persist(self):\n",
    "        self.dataset_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.train.to_csv(self.dataset_path / \"train.csv\", index=False)\n",
    "        self.test.to_csv(self.dataset_path / \"test.csv\", index=False)\n",
    "        self.next(self.end)\n",
    "\n",
    "    @step\n",
    "    def end(self):\n",
    "        print(f\"Split dataset saved to {self.dataset_dir}\")\n",
    "        print(f\"Shape of saved train set {self.train.drop(columns='target').shape}\")\n",
    "        print(f\"Shape of saved test set {self.test.drop(columns='target').shape}\")\n",
    "\n",
    "\n",
    "run = NBRunner(DataLoaderFlow, base_dir=\"./artifacts\").nbrun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cf076d-d9a2-49c8-93d5-f366e7feb575",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
